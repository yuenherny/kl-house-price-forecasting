{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning: Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Union\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skops.io as sio\n",
    "from tqdm import tqdm\n",
    "\n",
    "import helpers\n",
    "from helpers import (\n",
    "    CHARTS_DIR, RAW_DATA_DIR, IMPUTER_MODEL_DIR\n",
    ")\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>township</th>\n",
       "      <th>spa_date</th>\n",
       "      <th>address</th>\n",
       "      <th>building_type</th>\n",
       "      <th>tenure</th>\n",
       "      <th>floors</th>\n",
       "      <th>rooms</th>\n",
       "      <th>land_area</th>\n",
       "      <th>built_up</th>\n",
       "      <th>price_psf</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BANDAR BARU SRI PETALING</td>\n",
       "      <td>2023-06-09</td>\n",
       "      <td>✕✕✕, JALAN PIKRAMA</td>\n",
       "      <td>TERRACE HOUSE - INTERMEDIATE</td>\n",
       "      <td>LEASEHOLD</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>2,196 ft²</td>\n",
       "      <td>nan</td>\n",
       "      <td>342</td>\n",
       "      <td>750,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BANDAR BARU SRI PETALING</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>✕✕. ✕✕, JALAN PERLAK 3</td>\n",
       "      <td>TERRACE HOUSE - INTERMEDIATE</td>\n",
       "      <td>LEASEHOLD</td>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "      <td>753 ft²</td>\n",
       "      <td>nan</td>\n",
       "      <td>398</td>\n",
       "      <td>300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BANDAR BARU SRI PETALING</td>\n",
       "      <td>2023-05-29</td>\n",
       "      <td>✕✕ ✕, JALAN 12/149L</td>\n",
       "      <td>TERRACE HOUSE - INTERMEDIATE</td>\n",
       "      <td>LEASEHOLD</td>\n",
       "      <td>2½</td>\n",
       "      <td>nan</td>\n",
       "      <td>3,197 ft²</td>\n",
       "      <td>nan</td>\n",
       "      <td>188</td>\n",
       "      <td>600,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BANDAR BARU SRI PETALING</td>\n",
       "      <td>2023-05-25</td>\n",
       "      <td>✕✕. ✕✕✕, JALAN PASAI</td>\n",
       "      <td>TERRACE HOUSE - INTERMEDIATE</td>\n",
       "      <td>LEASEHOLD</td>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "      <td>753 ft²</td>\n",
       "      <td>nan</td>\n",
       "      <td>531</td>\n",
       "      <td>400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BANDAR BARU SRI PETALING</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>✕✕, JALAN SRI PETALING 5</td>\n",
       "      <td>SEMI-D</td>\n",
       "      <td>LEASEHOLD</td>\n",
       "      <td>2½</td>\n",
       "      <td>nan</td>\n",
       "      <td>4,801 ft²</td>\n",
       "      <td>nan</td>\n",
       "      <td>250</td>\n",
       "      <td>1,200,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294562</th>\n",
       "      <td>HERITAGE STATION HOTEL</td>\n",
       "      <td>1990-11-13</td>\n",
       "      <td>✕✕✕-✕✕✕, BB WANGSA MAJU</td>\n",
       "      <td>FLAT</td>\n",
       "      <td>LEASEHOLD</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>493 ft²</td>\n",
       "      <td>493 ft²</td>\n",
       "      <td>71</td>\n",
       "      <td>35,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294563</th>\n",
       "      <td>IDAMAN PUTERI</td>\n",
       "      <td>2005-01-10</td>\n",
       "      <td>✕✕-✕, JALAN GOMBAK</td>\n",
       "      <td>CONDOMINIUM</td>\n",
       "      <td>FREEHOLD</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1454 ft²</td>\n",
       "      <td>1454 ft²</td>\n",
       "      <td>150</td>\n",
       "      <td>218,025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294564</th>\n",
       "      <td>KELAB LE CHATEAU II</td>\n",
       "      <td>2008-02-25</td>\n",
       "      <td>✕-✕✕-✕, JALAN KIARA 3</td>\n",
       "      <td>CONDOMINIUM</td>\n",
       "      <td>FREEHOLD</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>593 ft²</td>\n",
       "      <td>593 ft²</td>\n",
       "      <td>194</td>\n",
       "      <td>115,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294565</th>\n",
       "      <td>MUTIARA SENTUL CONDOMINIUM</td>\n",
       "      <td>2009-08-10</td>\n",
       "      <td>✕-✕-✕, OFF JALAN SENTUL</td>\n",
       "      <td>APARTMENT</td>\n",
       "      <td>LEASEHOLD</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1193 ft²</td>\n",
       "      <td>1193 ft²</td>\n",
       "      <td>197</td>\n",
       "      <td>235,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294566</th>\n",
       "      <td>TAI CHEONG MANSION</td>\n",
       "      <td>1995-08-18</td>\n",
       "      <td>✕✕-✕✕-✕✕-✕, SENTUL</td>\n",
       "      <td>FLAT</td>\n",
       "      <td>LEASEHOLD</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1927 ft²</td>\n",
       "      <td>1927 ft²</td>\n",
       "      <td>269</td>\n",
       "      <td>518,296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>294567 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          township   spa_date                   address  \\\n",
       "0         BANDAR BARU SRI PETALING 2023-06-09        ✕✕✕, JALAN PIKRAMA   \n",
       "1         BANDAR BARU SRI PETALING 2023-06-01    ✕✕. ✕✕, JALAN PERLAK 3   \n",
       "2         BANDAR BARU SRI PETALING 2023-05-29       ✕✕ ✕, JALAN 12/149L   \n",
       "3         BANDAR BARU SRI PETALING 2023-05-25      ✕✕. ✕✕✕, JALAN PASAI   \n",
       "4         BANDAR BARU SRI PETALING 2023-05-22  ✕✕, JALAN SRI PETALING 5   \n",
       "...                            ...        ...                       ...   \n",
       "294562      HERITAGE STATION HOTEL 1990-11-13   ✕✕✕-✕✕✕, BB WANGSA MAJU   \n",
       "294563               IDAMAN PUTERI 2005-01-10        ✕✕-✕, JALAN GOMBAK   \n",
       "294564         KELAB LE CHATEAU II 2008-02-25     ✕-✕✕-✕, JALAN KIARA 3   \n",
       "294565  MUTIARA SENTUL CONDOMINIUM 2009-08-10   ✕-✕-✕, OFF JALAN SENTUL   \n",
       "294566          TAI CHEONG MANSION 1995-08-18        ✕✕-✕✕-✕✕-✕, SENTUL   \n",
       "\n",
       "                       building_type     tenure floors rooms  land_area  \\\n",
       "0       TERRACE HOUSE - INTERMEDIATE  LEASEHOLD      1   nan  2,196 ft²   \n",
       "1       TERRACE HOUSE - INTERMEDIATE  LEASEHOLD      2   nan    753 ft²   \n",
       "2       TERRACE HOUSE - INTERMEDIATE  LEASEHOLD     2½   nan  3,197 ft²   \n",
       "3       TERRACE HOUSE - INTERMEDIATE  LEASEHOLD      2   nan    753 ft²   \n",
       "4                             SEMI-D  LEASEHOLD     2½   nan  4,801 ft²   \n",
       "...                              ...        ...    ...   ...        ...   \n",
       "294562                          FLAT  LEASEHOLD      1     2    493 ft²   \n",
       "294563                   CONDOMINIUM   FREEHOLD      1     3   1454 ft²   \n",
       "294564                   CONDOMINIUM   FREEHOLD      1     3    593 ft²   \n",
       "294565                     APARTMENT  LEASEHOLD      1     2   1193 ft²   \n",
       "294566                          FLAT  LEASEHOLD      1     3   1927 ft²   \n",
       "\n",
       "        built_up price_psf      price  \n",
       "0            nan       342    750,000  \n",
       "1            nan       398    300,000  \n",
       "2            nan       188    600,000  \n",
       "3            nan       531    400,000  \n",
       "4            nan       250  1,200,000  \n",
       "...          ...       ...        ...  \n",
       "294562   493 ft²        71     35,000  \n",
       "294563  1454 ft²       150    218,025  \n",
       "294564   593 ft²       194    115,000  \n",
       "294565  1193 ft²       197    235,000  \n",
       "294566  1927 ft²       269    518,296  \n",
       "\n",
       "[294567 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transactions_encoded = pd.read_parquet(RAW_DATA_DIR / 'transactions_KL_ckpt4_encoded.parquet')\n",
    "df_transactions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294567 entries, 0 to 294566\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   township       294567 non-null  object        \n",
      " 1   spa_date       294567 non-null  datetime64[ns]\n",
      " 2   address        294546 non-null  object        \n",
      " 3   building_type  294567 non-null  object        \n",
      " 4   tenure         294567 non-null  object        \n",
      " 5   floors         294567 non-null  object        \n",
      " 6   rooms          294567 non-null  object        \n",
      " 7   land_area      294567 non-null  object        \n",
      " 8   built_up       294567 non-null  object        \n",
      " 9   price_psf      294567 non-null  object        \n",
      " 10  price          294567 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(10)\n",
      "memory usage: 24.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_transactions_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Remarks from Data Cleaning 1: Recap\n",
    "1. The following data cleaning steps has been performed:\n",
    "    - Removed address column\n",
    "    - Changed fraction to decimal\n",
    "    - Removed commas in numerical values\n",
    "    - Removed units in numerical values\n",
    "    - Removed exact duplicates\n",
    "    - Removed outliers using HDBSCAN, based on:\n",
    "        - Continuous variables: `land_area`, `built_up`, `price_psf` (3D)\n",
    "        - Ordinal variables: `floors`, `rooms` (2D)\n",
    "2. Investigated missing values in `built_up` and `rooms`\n",
    "    - Investigated correlation and association between features to determine which features to use to impute missing values\n",
    "3. Encoded features for imputation using one hot encoding\n",
    "\n",
    "Next, we should proceed to impute the missing values in `built_up` and `rooms`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing missing values\n",
    "\n",
    "Based on literature, the following imputation methods have been identified:\n",
    "1. Random forest imputation (Jager et al., 2021) for MCAR, MAR and MNAR data in various domain\n",
    "2. Multiple imputation by deterministic regression (Donlen, 2022) for MCAR data in real estate domain\n",
    "3. MissForest (Waljee et al., 2013) for MCAR data in medical domain\n",
    "4. Predictive mean matching, PMM (Heidt, 2019) for MAR data in medical domain\n",
    "5. KNN imputation (Jadhav et al., 2019) for MCAR, MAR and MNAR data in UCI dataset\n",
    "\n",
    "However, when filtered by domain (real estate), only three methods are identified:\n",
    "1. Random forest imputation\n",
    "2. KNN imputation\n",
    "3. Multiple imputation by deterministic regression\n",
    "\n",
    "These are machine learning approaches for imputation, where we treat the features with missing values as target variable and the features without missing values as independent variables. In order to obtain a better overview of the performance of the imputation methods, we use cross validation techniques:\n",
    "1. Split the dataset into train and test, where train are the data with labels and test are the data without labels\n",
    "2. Split the train dataset into train and validation\n",
    "3. Cross validate the train data:\n",
    "    - Create a pipeline with scaler and model\n",
    "    - Run cross validation with scoring\n",
    "    - Output both train and validation scores\n",
    "    - Return the pipeline and cross validation results\n",
    "4. Train and evaluate the model with validation data\n",
    "    - Train the pipeline with train data\n",
    "    - Predict the validation data\n",
    "    - Evaluate the model with validation data\n",
    "5. Evaluate the pipeline with validation data and print out the metrics\n",
    "6. Predict the test data\n",
    "7. Return the imputed dataset and the fitted pipeline\n",
    "\n",
    "References:\n",
    "- Jager et al. (2021): https://www.frontiersin.org/articles/10.3389/fdata.2021.693674/full\n",
    "- Donlen (2022): https://egrove.olemiss.edu/cgi/viewcontent.cgi?article=3744&context=hon_thesis\n",
    "- Waljee et al. (2013): https://bmjopen.bmj.com/content/3/8/e002847.citation-tools\n",
    "- Heidt (2019): https://dc.etsu.edu/cgi/viewcontent.cgi?article=5014&context=etd\n",
    "- Jadhav et al (2019): https://www.tandfonline.com/doi/full/10.1080/08839514.2019.1637138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    median_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "Model = Union[RandomForestRegressor, KNeighborsRegressor, RandomForestClassifier, KNeighborsClassifier]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing `built_up`\n",
    "\n",
    "The steps are:\n",
    "1. Remove `rooms` from the dataset as it has too many missing values\n",
    "2. Cross validate for `built_up` using:\n",
    "    - Random forest imputation\n",
    "    - KNN imputation\n",
    "3. Use the better model to impute `built_up`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'built_up'\n",
    "\n",
    "# Split the dataset into train and test, where train are the data with labels and test are the data without labels\n",
    "df_train = df_transactions_encoded[df_transactions_encoded[target].notna()].drop(columns=['rooms']).dropna()\n",
    "df_test = df_transactions_encoded[df_transactions_encoded[target].isna()].drop(columns=['rooms'])\n",
    "\n",
    "# Split the train dataset into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train.drop(columns=[target]), df_train[target], test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_with_pipeline(pipeline: Pipeline, X_train: np.ndarray, y_train: np.ndarray, task: str):\n",
    "\n",
    "    # Cross validate the train data\n",
    "    if task == 'regression':\n",
    "        scoring = ('r2', 'neg_root_mean_squared_error', 'neg_mean_absolute_percentage_error', 'neg_median_absolute_error')\n",
    "    elif task == 'classification':\n",
    "        scoring = ('accuracy', 'balanced_accuracy', 'f1', 'precision', 'recall', 'roc_auc')\n",
    "    else:\n",
    "        scoring = None\n",
    "\n",
    "    cv_results = cross_validate(pipeline, X_train, y_train, cv=5, scoring=scoring, return_train_score=True, n_jobs=4)\n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross validation for `built_up` using various techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for cross validation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_neg_root_mean_squared_error</th>\n",
       "      <th>train_neg_root_mean_squared_error</th>\n",
       "      <th>test_neg_mean_absolute_percentage_error</th>\n",
       "      <th>train_neg_mean_absolute_percentage_error</th>\n",
       "      <th>test_neg_median_absolute_error</th>\n",
       "      <th>train_neg_median_absolute_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1221.663679</td>\n",
       "      <td>11.541488</td>\n",
       "      <td>0.824293</td>\n",
       "      <td>0.973771</td>\n",
       "      <td>-357.268651</td>\n",
       "      <td>-136.370194</td>\n",
       "      <td>-4.652190e+15</td>\n",
       "      <td>-1.715361e+15</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1200.971122</td>\n",
       "      <td>9.686063</td>\n",
       "      <td>0.798486</td>\n",
       "      <td>0.975722</td>\n",
       "      <td>-379.701082</td>\n",
       "      <td>-131.454077</td>\n",
       "      <td>-5.023980e+15</td>\n",
       "      <td>-1.716289e+15</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233.660341</td>\n",
       "      <td>5.176621</td>\n",
       "      <td>0.854139</td>\n",
       "      <td>0.973795</td>\n",
       "      <td>-316.718636</td>\n",
       "      <td>-137.235538</td>\n",
       "      <td>-2.803023e+15</td>\n",
       "      <td>-1.677708e+15</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1236.589329</td>\n",
       "      <td>6.362303</td>\n",
       "      <td>0.844816</td>\n",
       "      <td>0.973615</td>\n",
       "      <td>-323.399088</td>\n",
       "      <td>-138.035930</td>\n",
       "      <td>-5.624051e+15</td>\n",
       "      <td>-1.652899e+15</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>285.546735</td>\n",
       "      <td>1.645550</td>\n",
       "      <td>0.784114</td>\n",
       "      <td>0.977032</td>\n",
       "      <td>-404.786024</td>\n",
       "      <td>-126.878500</td>\n",
       "      <td>-3.922569e+15</td>\n",
       "      <td>-1.042910e+15</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fit_time  score_time   test_r2  train_r2  \\\n",
       "0  1221.663679   11.541488  0.824293  0.973771   \n",
       "1  1200.971122    9.686063  0.798486  0.975722   \n",
       "2  1233.660341    5.176621  0.854139  0.973795   \n",
       "3  1236.589329    6.362303  0.844816  0.973615   \n",
       "4   285.546735    1.645550  0.784114  0.977032   \n",
       "\n",
       "   test_neg_root_mean_squared_error  train_neg_root_mean_squared_error  \\\n",
       "0                       -357.268651                        -136.370194   \n",
       "1                       -379.701082                        -131.454077   \n",
       "2                       -316.718636                        -137.235538   \n",
       "3                       -323.399088                        -138.035930   \n",
       "4                       -404.786024                        -126.878500   \n",
       "\n",
       "   test_neg_mean_absolute_percentage_error  \\\n",
       "0                            -4.652190e+15   \n",
       "1                            -5.023980e+15   \n",
       "2                            -2.803023e+15   \n",
       "3                            -5.624051e+15   \n",
       "4                            -3.922569e+15   \n",
       "\n",
       "   train_neg_mean_absolute_percentage_error  test_neg_median_absolute_error  \\\n",
       "0                             -1.715361e+15                            -0.0   \n",
       "1                             -1.716289e+15                            -0.0   \n",
       "2                             -1.677708e+15                            -0.0   \n",
       "3                             -1.652899e+15                            -0.0   \n",
       "4                             -1.042910e+15                            -0.0   \n",
       "\n",
       "   train_neg_median_absolute_error  \n",
       "0                             -0.0  \n",
       "1                             -0.0  \n",
       "2                             -0.0  \n",
       "3                             -0.0  \n",
       "4                             -0.0  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline with scaler and model\n",
    "rf_pipeline_built_up = Pipeline([('scaler', StandardScaler()), ('model', RandomForestRegressor(random_state=random_state, n_jobs=4))])\n",
    "\n",
    "# Cross validate the pipeline\n",
    "if not os.path.exists(IMPUTER_MODEL_DIR / 'cv_results_rf_built_up.joblib'):\n",
    "    cv_results_built_up = cross_validation_with_pipeline(rf_pipeline_built_up, X_train, y_train, 'regression')\n",
    "    joblib.dump(cv_results_built_up, IMPUTER_MODEL_DIR / 'cv_results_rf_built_up.joblib')\n",
    "else:\n",
    "    cv_results_built_up = joblib.load(IMPUTER_MODEL_DIR / 'cv_results_rf_built_up.joblib')\n",
    "\n",
    "pd.DataFrame(cv_results_built_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validation using random forest took 55m35s. The results wasn't that great:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.13 GiB for an array with shape (1896, 151045) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 724, in _fit_and_score\n    X_train, y_train = _safe_split(estimator, X, y, train)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 155, in _safe_split\n    X_subset = _safe_indexing(X, indices)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 353, in _safe_indexing\n    return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 195, in _pandas_indexing\n    return X.take(key, axis=axis)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\pandas\\core\\generic.py\", line 4068, in take\n    new_data = self._mgr.take(\n               ^^^^^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 877, in take\n    return self.reindex_indexer(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 670, in reindex_indexer\n    new_blocks = [\n                 ^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 671, in <listcomp>\n    blk.take_nd(\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 1061, in take_nd\n    new_values = algos.take_nd(\n                 ^^^^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py\", line 118, in take_nd\n    return _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py\", line 158, in _take_nd_ndarray\n    out = np.empty(out_shape, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 2.13 GiB for an array with shape (1896, 151045) and data type int64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Repos\\GitHub\\time-series-house-price-forecasting\\notebooks\\Transactions - 03 Data Cleaning.ipynb Cell 168\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repos/GitHub/time-series-house-price-forecasting/notebooks/Transactions%20-%2003%20Data%20Cleaning.ipynb#Y532sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Cross validate the pipeline\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repos/GitHub/time-series-house-price-forecasting/notebooks/Transactions%20-%2003%20Data%20Cleaning.ipynb#Y532sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(IMPUTER_MODEL_DIR \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcv_results_knn_built_up.joblib\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repos/GitHub/time-series-house-price-forecasting/notebooks/Transactions%20-%2003%20Data%20Cleaning.ipynb#Y532sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     cv_results_built_up \u001b[39m=\u001b[39m cross_validation_with_pipeline(knn_pipeline_built_up, X_train, y_train, \u001b[39m'\u001b[39;49m\u001b[39mregression\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repos/GitHub/time-series-house-price-forecasting/notebooks/Transactions%20-%2003%20Data%20Cleaning.ipynb#Y532sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     joblib\u001b[39m.\u001b[39mdump(cv_results_built_up, IMPUTER_MODEL_DIR \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcv_results_knn_built_up.joblib\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repos/GitHub/time-series-house-price-forecasting/notebooks/Transactions%20-%2003%20Data%20Cleaning.ipynb#Y532sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32md:\\Repos\\GitHub\\time-series-house-price-forecasting\\notebooks\\Transactions - 03 Data Cleaning.ipynb Cell 168\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repos/GitHub/time-series-house-price-forecasting/notebooks/Transactions%20-%2003%20Data%20Cleaning.ipynb#Y532sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repos/GitHub/time-series-house-price-forecasting/notebooks/Transactions%20-%2003%20Data%20Cleaning.ipynb#Y532sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     scoring \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Repos/GitHub/time-series-house-price-forecasting/notebooks/Transactions%20-%2003%20Data%20Cleaning.ipynb#Y532sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(pipeline, X_train, y_train, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, scoring\u001b[39m=\u001b[39;49mscoring, return_train_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repos/GitHub/time-series-house-price-forecasting/notebooks/Transactions%20-%2003%20Data%20Cleaning.ipynb#Y532sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results\n",
      "File \u001b[1;32md:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 309\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    310\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    311\u001b[0m         clone(estimator),\n\u001b[0;32m    312\u001b[0m         X,\n\u001b[0;32m    313\u001b[0m         y,\n\u001b[0;32m    314\u001b[0m         scorers,\n\u001b[0;32m    315\u001b[0m         train,\n\u001b[0;32m    316\u001b[0m         test,\n\u001b[0;32m    317\u001b[0m         verbose,\n\u001b[0;32m    318\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    319\u001b[0m         fit_params,\n\u001b[0;32m    320\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    321\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    322\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    323\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    324\u001b[0m     )\n\u001b[0;32m    325\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m indices\n\u001b[0;32m    326\u001b[0m )\n\u001b[0;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    330\u001b[0m \u001b[39m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32md:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[1;32md:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[39m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[39m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[39m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[39m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_error_fast()\n\u001b[0;32m   1700\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[39m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[39m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[39m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[39m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[39mif\u001b[39;00m error_job \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     error_job\u001b[39m.\u001b[39;49mget_result(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout)\n",
      "File \u001b[1;32md:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[39m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[39m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[39m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_return_or_raise()\n\u001b[0;32m    738\u001b[0m \u001b[39m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Repos\\GitHub\\time-series-house-price-forecasting\\venv\\Lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.13 GiB for an array with shape (1896, 151045) and data type int64"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with scaler and model\n",
    "knn_pipeline_built_up = Pipeline([('scaler', StandardScaler()), ('model', KNeighborsRegressor(n_jobs=4))])\n",
    "\n",
    "# Cross validate the pipeline\n",
    "if not os.path.exists(IMPUTER_MODEL_DIR / 'cv_results_knn_built_up.joblib'):\n",
    "    cv_results_built_up = cross_validation_with_pipeline(knn_pipeline_built_up, X_train, y_train, 'regression')\n",
    "    joblib.dump(cv_results_built_up, IMPUTER_MODEL_DIR / 'cv_results_knn_built_up.joblib')\n",
    "else:\n",
    "    cv_results_built_up = joblib.load(IMPUTER_MODEL_DIR / 'cv_results_knn_built_up.joblib')\n",
    "\n",
    "pd.DataFrame(cv_results_built_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validation using KNN took around 30m. The results wasn't that great:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check model performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(pipeline: Pipeline, X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray, task: str):\n",
    "\n",
    "    pipeline = pipeline.fit(X_train, y_train)\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "    print(\"Results for validation set:\")\n",
    "    if task == 'regression':\n",
    "        print(f\"R2 score: {r2_score(y_val, y_val_pred)}\")\n",
    "        print(f\"RMSE score: {sqrt(mean_squared_error(y_val, y_val_pred))}\")\n",
    "        print(f\"MAPE score: {mean_absolute_percentage_error(y_val, y_val_pred)}\")\n",
    "        print(f\"Median AE score: {median_absolute_error(y_val, y_val_pred)}\")\n",
    "    elif task == 'classification':\n",
    "        print(f\"Accuracy score: {accuracy_score(y_val, y_val_pred)}\")\n",
    "        print(f\"Balanced accuracy score: {balanced_accuracy_score(y_val, y_val_pred)}\")\n",
    "        print(f\"F1 score: {f1_score(y_val, y_val_pred)}\")\n",
    "        print(f\"Precision score: {precision_score(y_val, y_val_pred)}\")\n",
    "        print(f\"Recall score: {recall_score(y_val, y_val_pred)}\")\n",
    "        print(f\"ROC AUC score: {roc_auc_score(y_val, y_val_pred)}\")\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for validation set:\n",
      "R2 score: 0.8080609693884588\n",
      "RMSE score: 381.9099647033172\n",
      "MAPE score: 3890573305511410.5\n",
      "Median AE score: 0.0\n"
     ]
    }
   ],
   "source": [
    "rf_pipeline_built_up = validate_model(rf_pipeline_built_up, X_train, y_train, X_val, y_val, 'regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for validation set:\n",
      "R2 score: 0.8080609693884588\n",
      "RMSE score: 381.9099647033172\n",
      "MAPE score: 3890573305511410.5\n",
      "Median AE score: 0.0\n"
     ]
    }
   ],
   "source": [
    "knn_pipeline_built_up = validate_model(knn_pipeline_built_up, X_train, y_train, X_val, y_val, 'regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Impute `built_up` using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_model(pipeline: Pipeline, df_train: pd.DataFrame, df_test: pd.DataFrame, target: str):\n",
    "\n",
    "    y_test_pred = pipeline.predict(df_test)\n",
    "\n",
    "    df_test[target] = y_test_pred\n",
    "    df_imputed = pd.concat([df_train, df_test])\n",
    "\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 265270 entries, 253 to 76762\n",
      "Columns: 1904 entries, township_BANDAR BARU SRI PETALING to day\n",
      "dtypes: float64(5), int32(3), int64(1896)\n",
      "memory usage: 3.8 GB\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test.drop(columns=[target]).dropna()\n",
    "df_transactions_built_up_imputed = impute_with_model(rf_pipeline_built_up, df_train, df_test, target='built_up')\n",
    "df_transactions_built_up_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Join the imputed `built_up` data with the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 265270 entries, 253 to 76762\n",
      "Columns: 1905 entries, township_BANDAR BARU SRI PETALING to rooms\n",
      "dtypes: float64(6), int32(3), int64(1896)\n",
      "memory usage: 3.8 GB\n"
     ]
    }
   ],
   "source": [
    "df_transactions_built_up_imputed = df_transactions_built_up_imputed.join(df_transactions_encoded['rooms'])\n",
    "df_transactions_built_up_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rooms 29217\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in all columns\n",
    "for column in df_transactions_built_up_imputed.columns:\n",
    "    isna_count = df_transactions_built_up_imputed[column].isna().sum()\n",
    "    if isna_count > 0:\n",
    "        print(column, isna_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing `rooms`\n",
    "\n",
    "The steps are:\n",
    "1. Remove `rooms` with less than 5 samples so that CV can be performed\n",
    "2. Cross validate for `rooms` using:\n",
    "    - Random forest imputation\n",
    "    - KNN imputation\n",
    "3. Use the better model to impute `rooms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'rooms'\n",
    "\n",
    "# Split the dataset into train and test, where train are the data with labels and test are the data without labels\n",
    "df_train = df_transactions_built_up_imputed[df_transactions_built_up_imputed[target].notna()]\n",
    "df_train = df_train.groupby(target).filter(lambda x : len(x) > 5).dropna() # Drop rooms with less than 5 samples so that CV can be performed\n",
    "df_test = df_transactions_built_up_imputed[df_transactions_built_up_imputed[target].isna()]\n",
    "\n",
    "# Split the train dataset into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train.drop(columns=[target]), df_train[target], test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross validation for `built_up` using various techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for cross validation...\n",
      "fit_time: [294.35619926 324.97581124 287.97160578 324.420753   112.68837237]\n",
      "score_time: [10.89190555  8.71300745 13.77816391  8.32677245  2.44974971]\n",
      "test_r2: [0.35317275 0.43742745 0.48609734 0.45900313 0.50926415]\n",
      "train_r2: [0.99724572 0.99693067 0.99773382 0.99773768 0.99708288]\n",
      "test_neg_root_mean_squared_error: [-0.75864412 -0.72678535 -0.69506655 -0.69731085 -0.66276899]\n",
      "train_neg_root_mean_squared_error: [-0.0502898  -0.05273184 -0.04530322 -0.04552191 -0.05171767]\n",
      "test_neg_mean_absolute_percentage_error: [-1.92018098e+13 -1.65779600e+13 -1.59816305e+13 -1.81284167e+13\n",
      " -1.77706190e+13]\n",
      "train_neg_mean_absolute_percentage_error: [-1.78898849e+11 -1.78898849e+11 -1.78898849e+11 -1.19265899e+11\n",
      " -1.78898849e+11]\n",
      "test_neg_median_absolute_error: [-0. -0. -0. -0. -0.]\n",
      "train_neg_median_absolute_error: [-0. -0. -0. -0. -0.]\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with scaler and model\n",
    "rf_pipeline_rooms = Pipeline([('scaler', StandardScaler()), ('model', RandomForestClassifier(random_state=random_state, n_jobs=4))])\n",
    "\n",
    "# Cross validate the pipeline\n",
    "if not os.path.exists(IMPUTER_MODEL_DIR / 'cv_results_rf_rooms.joblib'):\n",
    "    cv_results_rooms = cross_validation_with_pipeline(rf_pipeline_rooms, X_train, y_train, 'regression')\n",
    "    joblib.dump(cv_results_rooms, IMPUTER_MODEL_DIR / 'cv_results_rf_rooms.joblib')\n",
    "else:\n",
    "    cv_results_rooms = joblib.load(IMPUTER_MODEL_DIR / 'cv_results_rf_rooms.joblib')\n",
    "\n",
    "pd.DataFrame(cv_results_rooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validation took 8m5s to complete. There are classes which has only one transactions, therefore the rooms with less than 5 counts were not included for cross validation.\n",
    "\n",
    "From the cross validation results, the model overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for cross validation...\n",
      "fit_time: [294.35619926 324.97581124 287.97160578 324.420753   112.68837237]\n",
      "score_time: [10.89190555  8.71300745 13.77816391  8.32677245  2.44974971]\n",
      "test_r2: [0.35317275 0.43742745 0.48609734 0.45900313 0.50926415]\n",
      "train_r2: [0.99724572 0.99693067 0.99773382 0.99773768 0.99708288]\n",
      "test_neg_root_mean_squared_error: [-0.75864412 -0.72678535 -0.69506655 -0.69731085 -0.66276899]\n",
      "train_neg_root_mean_squared_error: [-0.0502898  -0.05273184 -0.04530322 -0.04552191 -0.05171767]\n",
      "test_neg_mean_absolute_percentage_error: [-1.92018098e+13 -1.65779600e+13 -1.59816305e+13 -1.81284167e+13\n",
      " -1.77706190e+13]\n",
      "train_neg_mean_absolute_percentage_error: [-1.78898849e+11 -1.78898849e+11 -1.78898849e+11 -1.19265899e+11\n",
      " -1.78898849e+11]\n",
      "test_neg_median_absolute_error: [-0. -0. -0. -0. -0.]\n",
      "train_neg_median_absolute_error: [-0. -0. -0. -0. -0.]\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with scaler and model\n",
    "knn_pipeline_rooms = Pipeline([('scaler', StandardScaler()), ('model', KNeighborsClassifier(n_jobs=4))])\n",
    "\n",
    "# Cross validate the pipeline\n",
    "if not os.path.exists(IMPUTER_MODEL_DIR / 'cv_results_knn_rooms.joblib'):\n",
    "    cv_results_rooms = cross_validation_with_pipeline(knn_pipeline_rooms, X_train, y_train, 'regression')\n",
    "    joblib.dump(cv_results_rooms, IMPUTER_MODEL_DIR / 'cv_results_knn_rooms.joblib')\n",
    "else:\n",
    "    cv_results_rooms = joblib.load(IMPUTER_MODEL_DIR / 'cv_results_knn_rooms.joblib')\n",
    "\n",
    "pd.DataFrame(cv_results_rooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check model performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for validation set:\n",
      "R2 score: 0.5527093748549827\n",
      "RMSE score: 0.5689410437602002\n",
      "MAPE score: 15647437373178.346\n",
      "Median AE score: 0.0\n"
     ]
    }
   ],
   "source": [
    "rf_pipeline_rooms = validate_model(rf_pipeline_rooms, X_train, y_train, X_val, y_val, 'regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline took 2m27s to train and predict. However, the results were not good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for validation set:\n",
      "R2 score: 0.5527093748549827\n",
      "RMSE score: 0.5689410437602002\n",
      "MAPE score: 15647437373178.346\n",
      "Median AE score: 0.0\n"
     ]
    }
   ],
   "source": [
    "knn_pipeline_rooms = validate_model(knn_pipeline_rooms, X_train, y_train, X_val, y_val, 'regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline took 2m27s to train and predict. However, the results were not good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Impute `rooms` using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 265225 entries, 253 to 76762\n",
      "Columns: 1905 entries, township_BANDAR BARU SRI PETALING to rooms\n",
      "dtypes: float64(6), int32(3), int64(1896)\n",
      "memory usage: 3.8 GB\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test.drop(columns=[target]).dropna()\n",
    "df_transactions_rooms_imputed = impute_with_model(rf_pipeline_rooms, df_train, df_test, target='rooms')\n",
    "df_transactions_rooms_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_transactions_rooms_imputed.columns:\n",
    "    isna_count = df_transactions_rooms_imputed[column].isna().sum()\n",
    "    if isna_count > 0:\n",
    "        print(column, isna_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
